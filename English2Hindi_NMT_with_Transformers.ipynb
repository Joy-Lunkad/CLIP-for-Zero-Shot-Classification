{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English2Hindi_NMT_with_Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLNRjLR57iY1"
      },
      "source": [
        "#Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4ILs43J7nu-"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Input, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UMpD1sNwRut",
        "outputId": "48c9e565-43b3-463a-86f5-f919a7a2f0de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz6YVZNm2Jy9",
        "outputId": "0cf2247f-80d8-43b4-9c3c-34bf544d7648"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul  5 11:58:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1R3U2cO7pim"
      },
      "source": [
        "# Downloading and preparing the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaoiDLU3LLPl"
      },
      "source": [
        "## Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEFgfeXVEWIj"
      },
      "source": [
        "!cp '/content/drive/MyDrive/Colab Notebooks/kaggle.json' /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o5GPBMqKm9R",
        "outputId": "8ac18ead-c885-47ba-a753-5df4fb27bff3"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRZA9x7BK_Aa",
        "outputId": "ea95f7f9-83cb-41f4-d1d3-2514c8320106"
      },
      "source": [
        "!kaggle datasets download -d aiswaryaramachandran/hindienglish-corpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading hindienglish-corpora.zip to /content\n",
            " 65% 9.00M/13.9M [00:00<00:00, 17.0MB/s]\n",
            "100% 13.9M/13.9M [00:00<00:00, 20.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh-lr3xULEia"
      },
      "source": [
        "!unzip -q hindienglish-corpora.zip -d dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UJWEBp8LTbZ"
      },
      "source": [
        "df = pd.read_csv('/content/dataset/Hindi_English_Truncated_Corpus.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgPvujQ9LcVi",
        "outputId": "7b2f4a06-3ec9-42b5-872f-d7e985f511b3"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ted</td>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ted</td>\n",
              "      <td>I'd like to tell you about one such child,</td>\n",
              "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>This percentage is even greater than the perce...</td>\n",
              "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ted</td>\n",
              "      <td>what we really mean is that they're bad at not...</td>\n",
              "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>.The ending portion of these Vedas is called U...</td>\n",
              "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      source  ...                                     hindi_sentence\n",
              "0        ted  ...  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...\n",
              "1        ted  ...  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...\n",
              "2  indic2012  ...   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।\n",
              "3        ted  ...     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते\n",
              "4  indic2012  ...        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGQg6gqfOpjG",
        "outputId": "f1064474-bba0-4777-9f66-0d03db572814"
      },
      "source": [
        "pd.isnull(df).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "source              0\n",
              "english_sentence    2\n",
              "hindi_sentence      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YEQFuGAPOil"
      },
      "source": [
        "df = df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7ScKOSPNhWG"
      },
      "source": [
        "df.drop('source', axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZENgE6IkPl9e"
      },
      "source": [
        "df.drop_duplicates(inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZd0oRQDMJzG"
      },
      "source": [
        "## Preprocessing Sentence  \n",
        "\n",
        "\n",
        "1.   Add a start and end token to each sentence.\n",
        "2.   Clean the sentences by removing special characters.\n",
        "3.   Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4.   Pad each sentence to a maximum length.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRWOXuO3LeGi"
      },
      "source": [
        "df['english_sentence'] = df['english_sentence'].apply(lambda x : x.lower())\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x : x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9JlOEMWNwY3"
      },
      "source": [
        "df['english_sentence'] = df['english_sentence'].apply(lambda x : re.sub(\"'\", '', x))\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x : re.sub(\"'\", '', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUdBSMwcPdUI",
        "outputId": "66cc9c00-8090-4ea6-bf9a-a8aea195c65b"
      },
      "source": [
        "exclude = set(string.punctuation)\n",
        "df['english_sentence'] = df['english_sentence'].apply(lambda x : ''.join(ch for ch in x if ch not in exclude))\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x : ''.join(ch for ch in x if ch not in exclude))\n",
        "print(exclude)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'_', '*', ':', '@', '!', '&', '[', '+', '>', '`', '~', \"'\", ')', '?', '/', '|', '$', '(', ';', ',', '.', '}', '-', ']', '=', '\"', '<', '#', '{', '\\\\', '^', '%'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXtEAg1qVBj2"
      },
      "source": [
        "[maketrans explaination](https://www.programiz.com/python-programming/methods/string/maketrans)  \n",
        "[translate explaination](https://www.programiz.com/python-programming/methods/string/translate)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOW1n35dQWm8",
        "outputId": "4480ed91-e850-45a4-931f-aa1327481fd1"
      },
      "source": [
        "remove_digits = str.maketrans('', '', string.digits)\n",
        "df['english_sentence'] = df['english_sentence'].apply(lambda x : x.translate(remove_digits))\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "print(remove_digits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{48: None, 49: None, 50: None, 51: None, 52: None, 53: None, 54: None, 55: None, 56: None, 57: None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBdJVaD9X7Vc"
      },
      "source": [
        "Removing spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcbRznMkWPKc"
      },
      "source": [
        "df['english_sentence'] = df['english_sentence'].apply(lambda x : x.strip())\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: x.strip())\n",
        "df['english_sentence'] = df['english_sentence'].apply(lambda x : re.sub(\" +\", \" \", x))\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6hE33UuX_8u"
      },
      "source": [
        "Adding start and end tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcGJGYvQX_pK"
      },
      "source": [
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x : \"START_ \" + x + \" _END\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65vCf4hbX53L",
        "outputId": "39110bdf-27a5-48b0-c5f0-6bcc7bc74148"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>START_ राजनीतिज्ञों के पास जो कार्य करना चाहिए...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id like to tell you about one such child</td>\n",
              "      <td>START_ मई आपको ऐसे ही एक बच्चे के बारे में बता...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this percentage is even greater than the perce...</td>\n",
              "      <td>START_ यह प्रतिशत भारत में हिन्दुओं प्रतिशत से...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what we really mean is that theyre bad at not ...</td>\n",
              "      <td>START_ हम ये नहीं कहना चाहते कि वो ध्यान नहीं ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the ending portion of these vedas is called up...</td>\n",
              "      <td>START_ इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>the then governor of kashmir resisted transfer...</td>\n",
              "      <td>START_ कश्मीर के तत्कालीन गवर्नर ने इस हस्तांत...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>in this lies the circumstances of people befor...</td>\n",
              "      <td>START_ इसमें तुमसे पूर्व गुज़रे हुए लोगों के ह...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>and who are we to say even that they are wrong</td>\n",
              "      <td>START_ और हम होते कौन हैं यह कहने भी वाले कि व...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>“”global warming“” refer to warming caused in ...</td>\n",
              "      <td>START_ ग्लोबल वॉर्मिंग से आशय हाल ही के दशकों ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>you may want your child to go to a school that...</td>\n",
              "      <td>START_ हो सकता है कि आप चाहते हों कि आप का नऋर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>please ensure that you use the appropriate form</td>\n",
              "      <td>START_ कृपया यह सुनिश्चित कर लें कि आप सही फॉर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>category religious text</td>\n",
              "      <td>START_ श्रेणीधर्मग्रन्थ _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>this period summarily is pepped up with devotion</td>\n",
              "      <td>START_ यह काल समग्रतः भक्ति भावना से ओतप्रोत क...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>so there is some sort of justice</td>\n",
              "      <td>START_ तो वहाँ न्याय है _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>the first two were found unreliable and the pr...</td>\n",
              "      <td>START_ पहले दो को अविश्वसनीय मानकर बाकी पांच म...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>they had justified their educational policy of...</td>\n",
              "      <td>START_ कम संख़्या वाले उच्च एवं मध्यम श्रेणी क...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>and now at present the naturecure ayurvedic an...</td>\n",
              "      <td>START_ हाल में नेपाल के हस्पताल सामन्यतया आयुर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>parliament time frame is years and this will b...</td>\n",
              "      <td>START_ लोकसभा की कार्यावधि वर्ष है पर्ंतु इसे ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ii register courts empowered to try causes for...</td>\n",
              "      <td>START_ रजिस्टर न्यायालय जिन्हें न्यायाधीश द्वा...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>extreme weather due to increased mortality dis...</td>\n",
              "      <td>START_ बढ़ती हुई मौतों displacements और आर्थिक...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     english_sentence                                     hindi_sentence\n",
              "0   politicians do not have permission to do what ...  START_ राजनीतिज्ञों के पास जो कार्य करना चाहिए...\n",
              "1            id like to tell you about one such child  START_ मई आपको ऐसे ही एक बच्चे के बारे में बता...\n",
              "2   this percentage is even greater than the perce...  START_ यह प्रतिशत भारत में हिन्दुओं प्रतिशत से...\n",
              "3   what we really mean is that theyre bad at not ...  START_ हम ये नहीं कहना चाहते कि वो ध्यान नहीं ...\n",
              "4   the ending portion of these vedas is called up...  START_ इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता...\n",
              "5   the then governor of kashmir resisted transfer...  START_ कश्मीर के तत्कालीन गवर्नर ने इस हस्तांत...\n",
              "6   in this lies the circumstances of people befor...  START_ इसमें तुमसे पूर्व गुज़रे हुए लोगों के ह...\n",
              "7      and who are we to say even that they are wrong  START_ और हम होते कौन हैं यह कहने भी वाले कि व...\n",
              "8   “”global warming“” refer to warming caused in ...  START_ ग्लोबल वॉर्मिंग से आशय हाल ही के दशकों ...\n",
              "9   you may want your child to go to a school that...  START_ हो सकता है कि आप चाहते हों कि आप का नऋर...\n",
              "10    please ensure that you use the appropriate form  START_ कृपया यह सुनिश्चित कर लें कि आप सही फॉर...\n",
              "11                            category religious text                       START_ श्रेणीधर्मग्रन्थ _END\n",
              "12   this period summarily is pepped up with devotion  START_ यह काल समग्रतः भक्ति भावना से ओतप्रोत क...\n",
              "13                   so there is some sort of justice                       START_ तो वहाँ न्याय है _END\n",
              "14  the first two were found unreliable and the pr...  START_ पहले दो को अविश्वसनीय मानकर बाकी पांच म...\n",
              "15  they had justified their educational policy of...  START_ कम संख़्या वाले उच्च एवं मध्यम श्रेणी क...\n",
              "16  and now at present the naturecure ayurvedic an...  START_ हाल में नेपाल के हस्पताल सामन्यतया आयुर...\n",
              "17  parliament time frame is years and this will b...  START_ लोकसभा की कार्यावधि वर्ष है पर्ंतु इसे ...\n",
              "18  ii register courts empowered to try causes for...  START_ रजिस्टर न्यायालय जिन्हें न्यायाधीश द्वा...\n",
              "19  extreme weather due to increased mortality dis...  START_ बढ़ती हुई मौतों displacements और आर्थिक..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqug1_gfDlrd"
      },
      "source": [
        "df['len_eng_sentence'] = df['english_sentence'].apply(lambda x : len(x.split()))\n",
        "df['len_hindi_sentence'] = df['hindi_sentence'].apply(lambda x : len(x.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCgeM7FAEEXx",
        "outputId": "86f7bad1-3bd4-4492-ce30-4602eb2a3a13"
      },
      "source": [
        "print(df.iloc[82040]['english_sentence'])\n",
        "print(df.iloc[82040]['hindi_sentence'])\n",
        "print(df.iloc[82040])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mumbai city is situated in the western india of konkan regionand on ulhas river\n",
            "START_ मुंबई शहर भारत के पश्चिमी तट पर कोंकण तटीय क्षेत्र में उल्हास नदी के मुहाने पर स्थित है। _END\n",
            "english_sentence      mumbai city is situated in the western india o...\n",
            "hindi_sentence        START_ मुंबई शहर भारत के पश्चिमी तट पर कोंकण त...\n",
            "len_eng_sentence                                                     14\n",
            "len_hindi_sentence                                                   20\n",
            "Name: 83631, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvFHi45_EFqV",
        "outputId": "a56392cf-09b6-4c3c-bbd5-70b33ef792a9"
      },
      "source": [
        "print(df.shape)\n",
        "print(df[df['len_eng_sentence']>30].shape)\n",
        "df[df['len_hindi_sentence']>30].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(124825, 4)\n",
            "(12111, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18757, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCwYEx4IFE8V"
      },
      "source": [
        "lines = df[df['len_eng_sentence']<=30]\n",
        "lines = lines[lines['len_hindi_sentence']<=30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA9xSGsBFfHt",
        "outputId": "337634c0-6cde-4638-aa5b-dcff6d005f41"
      },
      "source": [
        "lines.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "      <th>len_eng_sentence</th>\n",
              "      <th>len_hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>START_ राजनीतिज्ञों के पास जो कार्य करना चाहिए...</td>\n",
              "      <td>12</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id like to tell you about one such child</td>\n",
              "      <td>START_ मई आपको ऐसे ही एक बच्चे के बारे में बता...</td>\n",
              "      <td>9</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this percentage is even greater than the perce...</td>\n",
              "      <td>START_ यह प्रतिशत भारत में हिन्दुओं प्रतिशत से...</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what we really mean is that theyre bad at not ...</td>\n",
              "      <td>START_ हम ये नहीं कहना चाहते कि वो ध्यान नहीं ...</td>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the ending portion of these vedas is called up...</td>\n",
              "      <td>START_ इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता...</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    english_sentence  ... len_hindi_sentence\n",
              "0  politicians do not have permission to do what ...  ...                 15\n",
              "1           id like to tell you about one such child  ...                 13\n",
              "2  this percentage is even greater than the perce...  ...                 11\n",
              "3  what we really mean is that theyre bad at not ...  ...                 13\n",
              "4  the ending portion of these vedas is called up...  ...                 10\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Trk6yzKfF4Wt",
        "outputId": "87bdfbb1-b878-451f-bd3a-7b24b7eb3bda"
      },
      "source": [
        "lines.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105194, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ibVb4xzIyu0"
      },
      "source": [
        "# Getting the data ready to feed it to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1wIrynACHlF"
      },
      "source": [
        "eng_tokenizer = Tokenizer()\n",
        "eng_tokenizer.fit_on_texts(lines['english_sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGTsg2ElDSLl"
      },
      "source": [
        "hindi_tokenizer = Tokenizer()\n",
        "hindi_tokenizer.fit_on_texts(lines['hindi_sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9G2uf7CGK89"
      },
      "source": [
        "input_words = list(eng_tokenizer.word_index.keys())\n",
        "target_words = list(hindi_tokenizer.word_index.keys())\n",
        "num_encoder_tokens = len(input_words)\n",
        "num_decoder_tokens = len(target_words) + 1 # for zero padding\n",
        "num_encoder_tokens, num_decoder_tokens\n",
        "latent_dim = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggZ0O22-HD-k",
        "outputId": "c8e4300b-714c-44d4-d55a-2e6d7d8fd760"
      },
      "source": [
        "lines = shuffle(lines)\n",
        "lines.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "      <th>len_eng_sentence</th>\n",
              "      <th>len_hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>64865</th>\n",
              "      <td>rajasthan state roadways corporationrstc opera...</td>\n",
              "      <td>START_ राजस्थान राज्य परिवहन निगम rstc की उत्त...</td>\n",
              "      <td>14</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34836</th>\n",
              "      <td>in movies and history</td>\n",
              "      <td>START_ फिल्म एवं साहित्य में _END</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65053</th>\n",
              "      <td>architect of main dome ismailak ismail khan wh...</td>\n",
              "      <td>START_ मुख्य गुम्बद का अभिकल्पक इस्माइल एकाइस्...</td>\n",
              "      <td>18</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81240</th>\n",
              "      <td>its not really what is realistic</td>\n",
              "      <td>START_ यह सच में वास्तविक नहीं है _END</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72718</th>\n",
              "      <td>getting these models to be adopted for drug di...</td>\n",
              "      <td>START_ दवाओं की खोज के लिए अपनाने की दिशा में ...</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94422</th>\n",
              "      <td>after her limp suicide attempt the begum suffe...</td>\n",
              "      <td>START_ आत्महत्या का प्रयास विफल हो जाने के बाद...</td>\n",
              "      <td>23</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118766</th>\n",
              "      <td>now what about the indus script</td>\n",
              "      <td>START_ अब सिंधु लिपि के बारे में क्या _END</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127155</th>\n",
              "      <td>residents were often given less than a months ...</td>\n",
              "      <td>START_ कभीकभी तो वहां के निवासियों की बिल्डिंग...</td>\n",
              "      <td>14</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124827</th>\n",
              "      <td>these episodes give us a glimpse of the kind o...</td>\n",
              "      <td>START_ इन घटनाओं से बसव के यहां आने वालों की ए...</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58433</th>\n",
              "      <td>you can get more information from the dss website</td>\n",
              "      <td>START_ आप को इस के बारे में अधिक जानकारी ढ्श्श...</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         english_sentence  ... len_hindi_sentence\n",
              "64865   rajasthan state roadways corporationrstc opera...  ...                 19\n",
              "34836                               in movies and history  ...                  6\n",
              "65053   architect of main dome ismailak ismail khan wh...  ...                 20\n",
              "81240                    its not really what is realistic  ...                  8\n",
              "72718   getting these models to be adopted for drug di...  ...                 11\n",
              "94422   after her limp suicide attempt the begum suffe...  ...                 24\n",
              "118766                    now what about the indus script  ...                  9\n",
              "127155  residents were often given less than a months ...  ...                 24\n",
              "124827  these episodes give us a glimpse of the kind o...  ...                 16\n",
              "58433   you can get more information from the dss website  ...                 17\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcAwGeD2IS01",
        "outputId": "6066fc04-29c4-414c-ebe7-7acf5fd82236"
      },
      "source": [
        "x, y = lines['english_sentence'][:], lines['hindi_sentence'][:]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((94674,), (10520,), (94674,), (10520,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5q1dpFoKUsF"
      },
      "source": [
        "x_train = eng_tokenizer.texts_to_sequences(x_train)\n",
        "y_train = hindi_tokenizer.texts_to_sequences(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9IwV47_KyuJ"
      },
      "source": [
        "x_train = pad_sequences(x_train, maxlen = 30, padding = 'post')\n",
        "y_train = pad_sequences(y_train, maxlen = 31, padding = 'post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USof-Xt5ikBW"
      },
      "source": [
        "x_test = eng_tokenizer.texts_to_sequences(x_test)\n",
        "y_test = hindi_tokenizer.texts_to_sequences(y_test)\n",
        "\n",
        "x_test = pad_sequences(x_test, maxlen = 30, padding = 'post')\n",
        "y_test = pad_sequences(y_test, maxlen = 31, padding = 'post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bsIpb5UMWEf"
      },
      "source": [
        "max_len_targ, max_length_inp = y_train.shape[1], x_train.shape[1]\n",
        "BUFFER_SIZE = len(x_train)\n",
        "BATCH_SIZE = 128\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "sequence_length = 30\n",
        "vocab_inp_size = len(eng_tokenizer.word_index) + 1\n",
        "vocab_tar_size = len(hindi_tokenizer.word_index) + 1\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(((x_train, y_train[:, :-1]), y_train[:, 1:])).shuffle(BUFFER_SIZE)\n",
        "train_ds = train_ds.batch(BATCH_SIZE, drop_remainder = True)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(((x_test, y_test[:, :-1]), y_test[:, 1:])).shuffle(BUFFER_SIZE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE, drop_remainder = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUXTPg49RFnq",
        "outputId": "124f1592-386b-42fb-a3dc-6d3b806e60c2"
      },
      "source": [
        "for (enc_inputs, decoder_inputs), targets in test_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {enc_inputs.shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {decoder_inputs.shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "    print(f'inputs[\"encoder_inputs\"] example: {enc_inputs[0]}')\n",
        "    print(f'inputs[\"decoder_inputs\"] example: {decoder_inputs[0]}')\n",
        "    print(f\"targets example: {targets[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (128, 30)\n",
            "inputs[\"decoder_inputs\"].shape: (128, 30)\n",
            "targets.shape: (128, 30)\n",
            "inputs[\"encoder_inputs\"] example: [  419  4531   348  1608  4616    32  7285    27   753    18 39645    22\n",
            "   419     5   419     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0]\n",
            "inputs[\"decoder_inputs\"] example: [    1  3358    85  5499  4541    17    59  4306    17    59     7 14968\n",
            "    17    59     7    12     8   197  1246    38  2785 43783    11     2\n",
            "     0     0     0     0     0     0]\n",
            "targets example: [ 3358    85  5499  4541    17    59  4306    17    59     7 14968    17\n",
            "    59     7    12     8   197  1246    38  2785 43783    11     2     0\n",
            "     0     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsTLlw884CEE"
      },
      "source": [
        "# Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cQeDq9J4H0X"
      },
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, seq_len, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=seq_len, output_dim=embed_dim)\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embed_tokens = self.emb(inputs)\n",
        "        embed_pos = self.pos_emb(positions)\n",
        "        return embed_tokens + embed_pos\n",
        "\n",
        "    def compute_mask(self, inputs, mask = None):\n",
        "        return tf.math.not_equal(inputs, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G7QuWUs9OoG"
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                                   key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([\n",
        "                                               layers.Dense(dense_dim, activation = 'relu'),\n",
        "                                               layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.ln1 = layers.LayerNormalization()\n",
        "        self.ln2 = layers.LayerNormalization()\n",
        "        \n",
        "    def call(self, inputs, mask = None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype='int32')\n",
        "        attention_out = self.attention(\n",
        "            query = inputs, value = inputs, key = inputs, attention_mask = padding_mask\n",
        "        )\n",
        "        proj_input = self.ln1(inputs + attention_out)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.ln2(proj_input + proj_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPfG3RlE7yY-"
      },
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
        "        )\n",
        "        self.ln1 = layers.LayerNormalization()\n",
        "        self.ln2 = layers.LayerNormalization()\n",
        "        self.ln3 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_att_mask(inputs) # 128, 30, 30\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.ln1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.ln2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.ln3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_att_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, seq_len = input_shape[0], input_shape[1]\n",
        "        i = tf.range(seq_len)[:, tf.newaxis]\n",
        "        j = tf.range(seq_len)\n",
        "        mask = tf.cast(i >= j, dtype='int32')\n",
        "        mask = tf.reshape(mask, (1, seq_len, seq_len))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "            tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis = 0\n",
        "            )\n",
        "        return tf.tile(mask, mult)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce2GjTlb9DDt"
      },
      "source": [
        "encoder_inputs = tf.keras.Input(shape=(sequence_length), dtype=\"int64\", name=\"encoder_inputs\") # 128, 30\n",
        "x = PositionalEmbedding(sequence_length, vocab_inp_size, embed_dim)(encoder_inputs) # 128, 30, 300\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x) # 128, 2048 \n",
        "encoder = Model(encoder_inputs, encoder_outputs) \n",
        "\n",
        "decoder_inputs = Input(shape=(sequence_length), dtype=\"int64\", name=\"decoder_inputs\") # 128, 30\n",
        "encoded_seq_inputs = Input(shape=(sequence_length, embed_dim), name=\"decoder_state_inputs\") # 128, 30, 300\n",
        "x = PositionalEmbedding(sequence_length, num_decoder_tokens, embed_dim)(decoder_inputs) # 128, 30, 300\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs) # #128, 30, 300\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_tar_size, activation=\"softmax\")(x)\n",
        "decoder = Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")\n",
        "transformer.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3GiRvcLL8Uc",
        "outputId": "eaf414b7-ed40-4b24-b9fa-a51bda2bdaa0"
      },
      "source": [
        "transformer.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding (Positiona (None, 30, 256)      14314752    encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder (Transforme (None, 30, 256)      3155456     positional_embedding[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, 30, 59564)    35823532    decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 53,293,740\n",
            "Trainable params: 53,293,740\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7LzE0zIQaRb",
        "outputId": "caa20291-539f-45ad-c963-d4d73614a2b7"
      },
      "source": [
        "transformer.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = transformer.fit(train_ds, epochs=15, validation_data=test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "739/739 [==============================] - 378s 508ms/step - loss: 3.2182 - accuracy: 0.6031 - val_loss: 2.7362 - val_accuracy: 0.6256\n",
            "Epoch 2/15\n",
            "739/739 [==============================] - 375s 507ms/step - loss: 2.5814 - accuracy: 0.6401 - val_loss: 2.4756 - val_accuracy: 0.6461\n",
            "Epoch 3/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 2.3133 - accuracy: 0.6594 - val_loss: 2.2997 - val_accuracy: 0.6608\n",
            "Epoch 4/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 2.0870 - accuracy: 0.6783 - val_loss: 2.1696 - val_accuracy: 0.6747\n",
            "Epoch 5/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 1.8830 - accuracy: 0.6980 - val_loss: 2.0764 - val_accuracy: 0.6870\n",
            "Epoch 6/15\n",
            "739/739 [==============================] - 374s 505ms/step - loss: 1.7030 - accuracy: 0.7177 - val_loss: 2.0238 - val_accuracy: 0.6989\n",
            "Epoch 7/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 1.5460 - accuracy: 0.7357 - val_loss: 2.0025 - val_accuracy: 0.7070\n",
            "Epoch 8/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 1.4125 - accuracy: 0.7516 - val_loss: 1.9769 - val_accuracy: 0.7139\n",
            "Epoch 9/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 1.2973 - accuracy: 0.7657 - val_loss: 1.9923 - val_accuracy: 0.7180\n",
            "Epoch 10/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 1.1965 - accuracy: 0.7787 - val_loss: 2.0269 - val_accuracy: 0.7210\n",
            "Epoch 11/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 1.1088 - accuracy: 0.7908 - val_loss: 2.1052 - val_accuracy: 0.7243\n",
            "Epoch 12/15\n",
            "739/739 [==============================] - 375s 507ms/step - loss: 1.0320 - accuracy: 0.8015 - val_loss: 2.1249 - val_accuracy: 0.7258\n",
            "Epoch 13/15\n",
            "739/739 [==============================] - 373s 505ms/step - loss: 0.9643 - accuracy: 0.8114 - val_loss: 2.1535 - val_accuracy: 0.7276\n",
            "Epoch 14/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 0.9042 - accuracy: 0.8205 - val_loss: 2.1908 - val_accuracy: 0.7277\n",
            "Epoch 15/15\n",
            "739/739 [==============================] - 374s 506ms/step - loss: 0.8500 - accuracy: 0.8288 - val_loss: 2.2991 - val_accuracy: 0.7292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYnOVnZQptCZ"
      },
      "source": [
        "transformer.save('english-to-hindi-translator-transformer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVzxjGyBp-7w"
      },
      "source": [
        "transformer.save_weights('translator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXxZv2keqL0u"
      },
      "source": [
        "!cp -r '/content/english-to-hindi-translator-transformer' '/content/drive/MyDrive/NLP/English to Hindi machine translation/transformer/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymzI0BiNqmg-"
      },
      "source": [
        "!cp '/content/translator_weights.h5' '/content/drive/MyDrive/NLP/English to Hindi machine translation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDJK8W3Df7ty"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNdWxENzdq4R"
      },
      "source": [
        "transformer.load_weights('/content/drive/MyDrive/NLP/English to Hindi machine translation/translator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3yzTVIgjyEv",
        "outputId": "f6afb16d-dc04-4222-cb3b-53ef84a4306e"
      },
      "source": [
        "def english_vectorization(sentence):\n",
        "    sample = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    sample = pad_sequences(sample, maxlen = 30, padding = 'post')\n",
        "    return sample\n",
        "\n",
        "def hindi_vectorization(sentence):\n",
        "    sample = hindi_tokenizer.texts_to_sequences([sentence])\n",
        "    sample = pad_sequences(sample, maxlen = 31, padding = 'post')\n",
        "    return sample\n",
        "\n",
        "english_sample = lines['english_sentence'][1]\n",
        "hindi_sample = lines['hindi_sentence'][1]\n",
        "\n",
        "print(english_sample)\n",
        "tokenized_sample = english_vectorization(english_sample)\n",
        "print(tokenized_sample)\n",
        "print('*' * 80)\n",
        "print(hindi_sample)\n",
        "tokenized_sample = hindi_vectorization(hindi_sample)\n",
        "print(tokenized_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id like to tell you about one such child\n",
            "[[1185   57    5  323   16   46   36  129  268    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "********************************************************************************\n",
            "START_ मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी _END\n",
            "[[   1 1092  126  156   22   12  225    3   84    4 1428 4467    2    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmjODy-XQsFj"
      },
      "source": [
        "hindi_vocab = target_words\n",
        "hindi_index_lookup = dict(zip(range(len(hindi_vocab)), hindi_vocab))\n",
        "max_decoded_sentence_length = 30\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = english_vectorization(input_sentence)\n",
        "    decoded_sentence = \"[start]\"\n",
        "    \n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = hindi_vectorization(decoded_sentence)[:, :-1]\n",
        "        #tokenized_target_sentence = hindi_vectorization(decoded_sentence)\n",
        "        #print(tokenized_target_sentence)\n",
        "        \n",
        "        predictions = transformer.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        predicted_word = np.argmax(predictions[0, i, :])\n",
        "        \n",
        "        #print(predicted_word)\n",
        "        sampled_token = hindi_tokenizer.sequences_to_texts([[predicted_word]])\n",
        "        #print(sampled_token)\n",
        "        #print(decoded_sentence)\n",
        "        if sampled_token[0] == \"end\":\n",
        "            break\n",
        "        decoded_sentence += \" \" + sampled_token[0]\n",
        "    decoded_sentence = decoded_sentence[8:]\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFoBFuFnkF7V"
      },
      "source": [
        "def translate(english_sentence):\n",
        "    hindi_translation = decode_sequence(english_sentence)\n",
        "    print(f'English Input : {english_sentence}')\n",
        "    print(f'Hindi Translation : {hindi_translation}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZFDfhkQm2VH",
        "outputId": "3504109d-96ca-44d4-f9a1-907ae19d1358"
      },
      "source": [
        "translate(english_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Input : id like to tell you about one such child\n",
            "Hindi Translation : मैं आपको एक बच्चे के बारे में बताना चाहूँगा\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2ld4RdSzT8S",
        "outputId": "d530e206-b4ce-4108-ca96-6da49872f410"
      },
      "source": [
        "for (enc_inps, dec_inps), outs in test_ds:\n",
        "    for example in range(3):\n",
        "        inps_example = [enc_inps[example:example+1,:], dec_inps[example:example+1, :]]\n",
        "        predictions = transformer.predict(inps_example)\n",
        "        sampled_tokens_index = np.argmax(predictions[0,:, :], axis=1)\n",
        "        pred = hindi_tokenizer.sequences_to_texts([sampled_tokens_index])[0][:-3]\n",
        "        targ = hindi_tokenizer.sequences_to_texts([dec_inps.numpy()[example]])[0][5:-3]\n",
        "        eng_ex = eng_tokenizer.sequences_to_texts([enc_inps.numpy()[example]])[0]\n",
        "\n",
        "        print(\"Input : \", eng_ex)\n",
        "        print(\"Target : \", targ) \n",
        "        print(\"Predicted : \", pred) \n",
        "        print()\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input :  at a parliamentary party meeting patil s “ impartiality ” stumped his audience ” if we want we can support poto\n",
            "Target :   संसदीय दल की एक बै क में पाटील ने यह कहकर सबको चकरा दिया कि हम चाहें तो पोटो का समर्थन कर सकते हैं चाहें तो विरोध \n",
            "Predicted :  एक दल के मुय वरिष् क में यह ही भी बात कि मालूम कि कि हम अर्जेंटीना या मीड़िया से पता दें लेते हैं end end क्या\n",
            "\n",
            "Input :  fish tenga\n",
            "Target :   फिश टेंगा \n",
            "Predicted :  फिश टेंगा \n",
            "\n",
            "Input :  religious texts of hindu religion has been divided into two parts shruti and smriti\n",
            "Target :   हिंदू धर्म के पवित्र ग्रन्थों को दो भागों में बाँटा गया है श्रुति और स्मृति। \n",
            "Predicted :  हिन्दू धर्म के अलावा ग्रन्थों को तीन भागों में बाँटा गया है। श्रुति और स्मृति। \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}